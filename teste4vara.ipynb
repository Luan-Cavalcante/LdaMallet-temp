{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874bf0ba-1d11-488b-91c6-b9cc280efcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- --------\n",
      "asttokens         2.0.7\n",
      "backcall          0.2.0\n",
      "cramjam           2.5.0\n",
      "debugpy           1.6.2\n",
      "decorator         5.1.1\n",
      "entrypoints       0.4\n",
      "executing         0.9.1\n",
      "fastparquet       0.8.1\n",
      "fsspec            2022.7.1\n",
      "funcy             1.17\n",
      "future            0.18.2\n",
      "gensim            3.8.3\n",
      "ipykernel         6.15.1\n",
      "ipython           8.4.0\n",
      "jedi              0.18.1\n",
      "Jinja2            3.1.2\n",
      "joblib            1.1.0\n",
      "jupyter-client    7.3.4\n",
      "jupyter-core      4.11.1\n",
      "MarkupSafe        2.1.1\n",
      "matplotlib-inline 0.1.3\n",
      "nest-asyncio      1.5.5\n",
      "numexpr           2.8.3\n",
      "numpy             1.23.1\n",
      "packaging         21.3\n",
      "pandas            1.4.3\n",
      "parso             0.8.3\n",
      "pexpect           4.8.0\n",
      "pickleshare       0.7.5\n",
      "pip               22.2.2\n",
      "prompt-toolkit    3.0.30\n",
      "psutil            5.9.1\n",
      "ptyprocess        0.7.0\n",
      "pure-eval         0.2.2\n",
      "Pygments          2.12.0\n",
      "pyLDAvis          3.3.1\n",
      "pyparsing         3.0.9\n",
      "python-dateutil   2.8.2\n",
      "pytz              2022.2\n",
      "pyzmq             23.2.1\n",
      "scikit-learn      1.1.2\n",
      "scipy             1.9.0\n",
      "setuptools        56.0.0\n",
      "six               1.16.0\n",
      "sklearn           0.0\n",
      "smart-open        6.0.0\n",
      "stack-data        0.3.0\n",
      "threadpoolctl     3.1.0\n",
      "tornado           6.2\n",
      "traitlets         5.3.0\n",
      "wcwidth           0.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56d3d8-29df-4c08-ae92-ac98c55ce768",
   "metadata": {},
   "outputs": [],
   "source": [
    "### aqui é para os testes \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "### Isso é para a classe do professor Thiago\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from gensim.matutils import Sparse2Corpus, corpus2dense\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "from ldamallet import LdaMalletHandler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b37d98d-366c-4598-8e00-9478e727a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/anasofia/Documents/Projetos/AILab/ALEI-1a/dados/4a_secao/ds.parquet.gzip'\n",
    "\n",
    "docs = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16664176-8a65-4fec-aa15-f6052bec812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(strip_accents = 'unicode',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.7, \n",
    "                                min_df = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4000e88-1890-4e70-a86f-c43fd8ccf3bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Projetos/AILab/ALEI-1a/LdaMallet-temp/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:142\u001b[0m, in \u001b[0;36mstrip_accents_unicode\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# If `s` is ASCII-compatible, then it does not contain any accented\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# characters and we can avoid an expensive list comprehension\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mASCII\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\xba' in position 3: ordinal not in range(128)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projetos/AILab/ALEI-1a/LdaMallet-temp/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projetos/AILab/ALEI-1a/LdaMallet-temp/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Documents/Projetos/AILab/ALEI-1a/LdaMallet-temp/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/Documents/Projetos/AILab/ALEI-1a/LdaMallet-temp/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43maccent_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/Documents/Projetos/AILab/ALEI-1a/LdaMallet-temp/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:145\u001b[0m, in \u001b[0;36mstrip_accents_unicode\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     normalized \u001b[38;5;241m=\u001b[39m \u001b[43municodedata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNFKD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m normalized \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unicodedata\u001b[38;5;241m.\u001b[39mcombining(c)])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = vect.fit_transform(docs['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43527f-90dd-4f4a-bcb0-7548c0a2bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_mallet_binary = \"/home/anasofia/Mallet/bin/mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa017f-95cf-42a2-a7a0-0240abcf4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaMalletHandler(n_components=20,\n",
    "                            mallet_path=path_to_mallet_binary,\n",
    "                            iterations=100,\n",
    "                            vectorizer=vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd1914-1756-4f6a-8f0a-c2a6a54d5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ldamodel.vect2gensim(vect, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9ed46-7372-4264-a477-5d8a8d4e1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaMallet(path_to_mallet_binary,\n",
    "          iterations=100,\n",
    "          corpus=a,\n",
    "          num_topics=20,\n",
    "          id2word=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9cda87-0783-46d4-9049-40f7f7fe7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = model.fstate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ab9c3-30fa-4a75-b944-b6d87a1c52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe1e54-7ed0-4117-9f53-d422121fe7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfrbrowser import dfrBrowserConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199f076-949c-46bd-8dfd-5b23c197f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = docs['text']\n",
    "data.to_csv('meta.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0682b-d693-432d-ba55-397c682798d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = dfrBrowserConverter(ldamodel, path='dfr-browser/data')\n",
    "conv.generate_files('aaaaaa', 'BBBBBBBBJDSAFKLDSJAFKJLS', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f123d-466f-4954-92fe-a973b1a901e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
